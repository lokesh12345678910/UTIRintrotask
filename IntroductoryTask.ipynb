{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a list of all reviews\n",
    "def combine_reviews(mypath):\n",
    "    #calls method to make a list of file addresses for each review\n",
    "    review_locations = find_locations(mypath)\n",
    "    #calls method to make list of all reviews\n",
    "    return make_list_of_reviews(mypath, review_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of file addresses for each review\n",
    "def find_locations(mypath):\n",
    "    from os import listdir\n",
    "    from os.path import isfile, join\n",
    "    review_locations = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "    return review_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#given a list of all review file addresses, makes a list of all reviews\n",
    "def make_list_of_reviews(mypath, review_locations):\n",
    "    reviews = []\n",
    "    for file in review_locations:\n",
    "        f = open(mypath + \"/\" + file,encoding='utf-8')\n",
    "        current_review = f.read()\n",
    "        #make lower case\n",
    "        current_review = current_review.lower()\n",
    "        #remove punctuation\n",
    "        from string import punctuation\n",
    "        current_review = ''.join([c for c in current_review if c not in punctuation])\n",
    "        reviews.append(current_review)\n",
    "        f.close()    \n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_train_reviews = combine_reviews(\"/Users/pugal/LeaseResearch/aclImdb/train/pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bromwell high is a cartoon comedy it ran at the same time as some other programs about school life such as teachers my 35 years in the teaching profession lead me to believe that bromwell highs satire is much closer to reality than is teachers the scramble to survive financially the insightful students who can see right through their pathetic teachers pomp the pettiness of the whole situation all remind me of the schools i knew and their students when i saw the episode in which a student repeatedly tried to burn down the school i immediately recalled  at  high a classic line inspector im here to sack one of your teachers student welcome to bromwell high i expect that many adults of my age think that bromwell high is far fetched what a pity that it isnt',\n",
       " 'homelessness or houselessness as george carlin stated has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school work or vote for the matter most people think of the homeless as just a lost cause while worrying about things such as racism the war on iraq pressuring kids to succeed technology the elections inflation or worrying if theyll be next to end up on the streetsbr br but what if you were given a bet to live on the streets for a month without the luxuries you once had from a home the entertainment sets a bathroom pictures on the wall a computer and everything you once treasure to see what its like to be homeless that is goddard bolts lessonbr br mel brooks who directs who stars as bolt plays a rich man who has everything in the world until deciding to make a bet with a sissy rival jeffery tambor to see if he can live in the streets for thirty days without the luxuries if bolt succeeds he can do what he wants with a future project of making more buildings the bets on where bolt is thrown on the street with a bracelet on his leg to monitor his every move where he cant step off the sidewalk hes given the nickname pepto by a vagrant after its written on his forehead where bolt meets other characters including a woman by the name of molly lesley ann warren an exdancer who got divorce before losing her home and her pals sailor howard morris and fumes teddy wilson who are already used to the streets theyre survivors bolt isnt hes not used to reaching mutual agreements like he once did when being rich where its fight or flight kill or be killedbr br while the love connection between molly and bolt wasnt necessary to plot i found life stinks to be one of mel brooks observant films where prior to being a comedy it shows a tender side compared to his slapstick work such as blazing saddles young frankenstein or spaceballs for the matter to show what its like having something valuable before losing it the next day or on the other hand making a stupid bet like all rich people do when they dont know what to do with their money maybe they should give it to the homeless instead of using it like monopoly moneybr br or maybe this film will inspire you to help others',\n",
       " 'brilliant overacting by lesley ann warren best dramatic hobo lady i have ever seen and love scenes in clothes warehouse are second to none the corn on face is a classic as good as anything in blazing saddles the take on lawyers is also superb after being accused of being a turncoat selling out his boss and being dishonest the lawyer of pepto bolt shrugs indifferently im a lawyer he says three funny words jeffrey tambor a favorite from the later larry sanders show is fantastic here too as a mad millionaire who wants to crush the ghetto his character is more malevolent than usual the hospital scene and the scene where the homeless invade a demolition site are alltime classics look for the legs scene and the two big diggers fighting one bleeds this movie gets better each time i see it which is quite often']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_train_reviews[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_train_reviews = combine_reviews(\"/Users/pugal/LeaseResearch/aclImdb/train/neg\")\n",
    "negative_train_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['story of a man who has unnatural feelings for a pig starts out with a opening scene that is a terrific example of absurd comedy a formal orchestra audience is turned into an insane violent mob by the crazy chantings of its singers unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting even those from the era should be turned off the cryptic dialogue would make shakespeare seem easy to a third grader on a technical level its better than you might think with some good cinematography by future great vilmos zsigmond future stars sally kirkland and frederic forrest can be seen briefly',\n",
       " 'airport 77 starts as a brand new luxury 747 plane is loaded up with valuable paintings  such belonging to rich businessman philip stevens james stewart who is flying them  a bunch of vips to his estate in preparation of it being opened to the public as a museum also on board is stevens daughter julie kathleen quinlan  her son the luxury jetliner takes off as planned but midair the plane is hijacked by the copilot chambers robert foxworth  his two accomplices banker monte markham  wilson michael pataki who knock the passengers  crew out with sleeping gas they plan to steal the valuable cargo  land on a disused plane strip on an isolated island but while making his descent chambers almost hits an oil rig in the ocean  loses control of the plane sending it crashing into the sea where it sinks to the bottom right bang in the middle of the bermuda triangle with air in short supply water leaking in  having flown over 200 miles off course the problems mount for the survivors as they await help with time fast running outbr br also known under the slightly different tile airport 1977 this second sequel to the smashhit disaster thriller airport 1970 was directed by jerry jameson  while once again like its predecessors i cant say airport 77 is any sort of forgotten classic it is entertaining although not necessarily for the right reasons out of the three airport films i have seen so far i actually liked this one the best just it has my favourite plot of the three with a nice midair hijacking  then the crashing didnt he see the oil rig  sinking of the 747 maybe the makers were trying to cross the original airport with another popular disaster flick of the period the poseidon adventure 1972  submerged is where it stays until the end with a stark dilemma facing those trapped inside either suffocate when the air runs out or drown as the 747 floods or if any of the doors are opened  its a decent idea that could have made for a great little disaster flick but bad unsympathetic characters dull dialogue lethargic setpieces  a real lack of danger or suspense or tension means this is a missed opportunity while the rather sluggish plot keeps one entertained for 108 odd minutes not that much happens after the plane sinks  theres not as much urgency as i thought there should have been even when the navy become involved things dont pick up that much with a few shots of huge ships  helicopters flying about but theres just something lacking here george kennedy as the jinxed airline worker joe patroni is back but only gets a couple of scenes  barely even says anything preferring to just look worried in the backgroundbr br the home video  theatrical version of airport 77 run 108 minutes while the us tv versions add an extra hour of footage including a new opening credits sequence many more scenes with george kennedy as patroni flashbacks to flesh out characters longer rescue scenes  the discovery or another couple of dead bodies including the navigator while i would like to see this extra footage i am not sure i could sit through a near three hour cut of airport 77 as expected the film has dated badly with horrible fashions  interior design choices i will say no more other than the toy plane model effects arent great either along with the other two airport sequels this takes pride of place in the razzie awards hall of shame although i can think of lots of worse films than this so i reckon thats a little harsh the action scenes are a little dull unfortunately the pace is slow  not much excitement or tension is generated which is a shame as i reckon this could have been a pretty good film if made properlybr br the production values are alright if nothing spectacular the acting isnt great two time oscar winner jack lemmon has said since it was a mistake to star in this one time oscar winner james stewart looks old  frail also one time oscar winner lee grant looks drunk while sir christopher lee is given little to do  there are plenty of other familiar faces to look out for toobr br airport 77 is the most disaster orientated of the three airport films so far  i liked the ideas behind it even if they were a bit silly the production  bland direction doesnt help though  a film about a sunken plane just shouldnt be this boring or lethargic followed by the concorde  airport 79 1979',\n",
       " 'this film lacked something i couldnt put my finger on at first charisma on the part of the leading actress this inevitably translated to lack of chemistry when she shared the screen with her leading man even the romantic scenes came across as being merely the actors at play it could very well have been the director who miscalculated what he needed from the actors i just dont knowbr br but could it have been the screenplay just exactly who was the chef in love with he seemed more enamored of his culinary skills and restaurant and ultimately of himself and his youthful exploits than of anybody or anything else he never convinced me he was in love with the princessbr br i was disappointed in this movie but dont forget it was nominated for an oscar so judge for yourself']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_train_reviews[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_reviews = positive_train_reviews+negative_train_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bromwell high is a cartoon comedy it ran at the same time as some other programs about school life such as teachers my 35 years in the teaching profession lead me to believe that bromwell highs satire is much closer to reality than is teachers the scramble to survive financially the insightful students who can see right through their pathetic teachers pomp the pettiness of the whole situation all remind me of the schools i knew and their students when i saw the episode in which a student repeatedly tried to burn down the school i immediately recalled  at  high a classic line inspector im here to sack one of your teachers student welcome to bromwell high i expect that many adults of my age think that bromwell high is far fetched what a pity that it isnt',\n",
       " 'homelessness or houselessness as george carlin stated has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school work or vote for the matter most people think of the homeless as just a lost cause while worrying about things such as racism the war on iraq pressuring kids to succeed technology the elections inflation or worrying if theyll be next to end up on the streetsbr br but what if you were given a bet to live on the streets for a month without the luxuries you once had from a home the entertainment sets a bathroom pictures on the wall a computer and everything you once treasure to see what its like to be homeless that is goddard bolts lessonbr br mel brooks who directs who stars as bolt plays a rich man who has everything in the world until deciding to make a bet with a sissy rival jeffery tambor to see if he can live in the streets for thirty days without the luxuries if bolt succeeds he can do what he wants with a future project of making more buildings the bets on where bolt is thrown on the street with a bracelet on his leg to monitor his every move where he cant step off the sidewalk hes given the nickname pepto by a vagrant after its written on his forehead where bolt meets other characters including a woman by the name of molly lesley ann warren an exdancer who got divorce before losing her home and her pals sailor howard morris and fumes teddy wilson who are already used to the streets theyre survivors bolt isnt hes not used to reaching mutual agreements like he once did when being rich where its fight or flight kill or be killedbr br while the love connection between molly and bolt wasnt necessary to plot i found life stinks to be one of mel brooks observant films where prior to being a comedy it shows a tender side compared to his slapstick work such as blazing saddles young frankenstein or spaceballs for the matter to show what its like having something valuable before losing it the next day or on the other hand making a stupid bet like all rich people do when they dont know what to do with their money maybe they should give it to the homeless instead of using it like monopoly moneybr br or maybe this film will inspire you to help others',\n",
       " 'brilliant overacting by lesley ann warren best dramatic hobo lady i have ever seen and love scenes in clothes warehouse are second to none the corn on face is a classic as good as anything in blazing saddles the take on lawyers is also superb after being accused of being a turncoat selling out his boss and being dishonest the lawyer of pepto bolt shrugs indifferently im a lawyer he says three funny words jeffrey tambor a favorite from the later larry sanders show is fantastic here too as a mad millionaire who wants to crush the ghetto his character is more malevolent than usual the hospital scene and the scene where the homeless invade a demolition site are alltime classics look for the legs scene and the two big diggers fighting one bleeds this movie gets better each time i see it which is quite often']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_reviews[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive training reviews:  12500\n",
      "Number of negative training reviews:  12500\n",
      "Total number of training reviews:  25000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of positive training reviews: \",len(positive_train_reviews))\n",
    "print(\"Number of negative training reviews: \", len(negative_train_reviews))\n",
    "print(\"Total number of training reviews: \", len(all_train_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding reviews\n",
    "def encode_reviews(positive_reviews, negative_reviews):\n",
    "    #combine negative and positive reviews into one lsit\n",
    "    all_reviews = positive_reviews + negative_reviews\n",
    "    sorted_words = sort_words_by_FreqDist(all_reviews)\n",
    "    vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}\n",
    "    reviews_int = encode_words_as_integers(all_reviews, vocab_to_int)\n",
    "    return reviews_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_words_by_FreqDist(all_reviews):\n",
    "    import nltk\n",
    "    all_text2 = ''.join(all_reviews)\n",
    "    words = all_text2.split()\n",
    "    #make an freqDist object to count all word frequencies\n",
    "    fd = nltk.FreqDist(words)\n",
    "    #use freqDist object to make a dictionary of words ordered by word frequencies\n",
    "    #len(words) ensures all words are ordered\n",
    "    sorted_words = fd.most_common(len(words))\n",
    "    return sorted_words    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_words_as_integers(all_reviews, vocab_to_int):\n",
    "    reviews_int = []\n",
    "    for review in all_reviews:\n",
    "        #for every word, find corresponding integer in vocab_to_int encoding\n",
    "        r = [vocab_to_int[w] for w in review.split() if w in vocab_to_int]\n",
    "        reviews_int.append(r)\n",
    "    return reviews_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(num_positive, num_negative):\n",
    "    encoded_labels = []\n",
    "    positive = 1\n",
    "    negative = 0\n",
    "    #recall first 12500 reviews in allreviews were positive\n",
    "    #second 12500 reviews were negative\n",
    "    for x in range(num_positive):\n",
    "        encoded_labels.append(positive)\n",
    "    for x in range(num_negative):\n",
    "        encoded_labels.append(negative)\n",
    "    return encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recall first 12500 reviews in allreviews were positive\n",
    "# second 12500 reviews were negative\n",
    "train_labels = encode_labels(12500, 12500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_train_reviews = combine_reviews(\"/Users/pugal/LeaseResearch/aclImdb/train/pos\")\n",
    "negative_train_reviews = combine_reviews(\"/Users/pugal/LeaseResearch/aclImdb/train/neg\")\n",
    "encoded_train_reviews = encode_reviews(positive_train_reviews, negative_train_reviews)\n",
    "encoded_train_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[24371,\n",
       "  320,\n",
       "  6,\n",
       "  3,\n",
       "  1079,\n",
       "  220,\n",
       "  8,\n",
       "  2072,\n",
       "  30,\n",
       "  1,\n",
       "  166,\n",
       "  62,\n",
       "  14,\n",
       "  46,\n",
       "  80,\n",
       "  5722,\n",
       "  42,\n",
       "  399,\n",
       "  119,\n",
       "  135,\n",
       "  14,\n",
       "  4852,\n",
       "  55,\n",
       "  4947,\n",
       "  147,\n",
       "  7,\n",
       "  1,\n",
       "  4908,\n",
       "  5992,\n",
       "  474,\n",
       "  69,\n",
       "  5,\n",
       "  253,\n",
       "  11,\n",
       "  24371,\n",
       "  17221,\n",
       "  1963,\n",
       "  6,\n",
       "  72,\n",
       "  2356,\n",
       "  5,\n",
       "  632,\n",
       "  70,\n",
       "  6,\n",
       "  4852,\n",
       "  1,\n",
       "  26417,\n",
       "  5,\n",
       "  2021,\n",
       "  11069,\n",
       "  1,\n",
       "  5846,\n",
       "  1422,\n",
       "  35,\n",
       "  68,\n",
       "  67,\n",
       "  206,\n",
       "  140,\n",
       "  64,\n",
       "  1228,\n",
       "  4852,\n",
       "  21261,\n",
       "  1,\n",
       "  44658,\n",
       "  4,\n",
       "  1,\n",
       "  214,\n",
       "  901,\n",
       "  31,\n",
       "  2898,\n",
       "  69,\n",
       "  4,\n",
       "  1,\n",
       "  4668,\n",
       "  10,\n",
       "  661,\n",
       "  2,\n",
       "  64,\n",
       "  1422,\n",
       "  52,\n",
       "  10,\n",
       "  207,\n",
       "  1,\n",
       "  382,\n",
       "  7,\n",
       "  59,\n",
       "  3,\n",
       "  1470,\n",
       "  3605,\n",
       "  767,\n",
       "  5,\n",
       "  3555,\n",
       "  187,\n",
       "  1,\n",
       "  399,\n",
       "  10,\n",
       "  1192,\n",
       "  15181,\n",
       "  30,\n",
       "  320,\n",
       "  3,\n",
       "  353,\n",
       "  361,\n",
       "  2982,\n",
       "  144,\n",
       "  133,\n",
       "  5,\n",
       "  9212,\n",
       "  28,\n",
       "  4,\n",
       "  122,\n",
       "  4852,\n",
       "  1470,\n",
       "  2500,\n",
       "  5,\n",
       "  24371,\n",
       "  320,\n",
       "  10,\n",
       "  510,\n",
       "  11,\n",
       "  104,\n",
       "  1476,\n",
       "  4,\n",
       "  55,\n",
       "  582,\n",
       "  102,\n",
       "  11,\n",
       "  24371,\n",
       "  320,\n",
       "  6,\n",
       "  231,\n",
       "  8838,\n",
       "  48,\n",
       "  3,\n",
       "  2305,\n",
       "  11,\n",
       "  8,\n",
       "  203],\n",
       " [26421,\n",
       "  40,\n",
       "  59564,\n",
       "  14,\n",
       "  723,\n",
       "  20113,\n",
       "  3360,\n",
       "  43,\n",
       "  73,\n",
       "  33,\n",
       "  1845,\n",
       "  15,\n",
       "  147,\n",
       "  17,\n",
       "  109,\n",
       "  3,\n",
       "  1284,\n",
       "  5,\n",
       "  325,\n",
       "  142,\n",
       "  20,\n",
       "  1,\n",
       "  885,\n",
       "  11,\n",
       "  65,\n",
       "  279,\n",
       "  1138,\n",
       "  398,\n",
       "  35,\n",
       "  116,\n",
       "  277,\n",
       "  36,\n",
       "  165,\n",
       "  5,\n",
       "  399,\n",
       "  161,\n",
       "  40,\n",
       "  2279,\n",
       "  15,\n",
       "  1,\n",
       "  549,\n",
       "  86,\n",
       "  81,\n",
       "  102,\n",
       "  4,\n",
       "  1,\n",
       "  3204,\n",
       "  14,\n",
       "  41,\n",
       "  3,\n",
       "  410,\n",
       "  1071,\n",
       "  131,\n",
       "  8214,\n",
       "  42,\n",
       "  178,\n",
       "  135,\n",
       "  14,\n",
       "  3095,\n",
       "  1,\n",
       "  328,\n",
       "  20,\n",
       "  4909,\n",
       "  29053,\n",
       "  326,\n",
       "  5,\n",
       "  3220,\n",
       "  2101,\n",
       "  1,\n",
       "  22694,\n",
       "  21262,\n",
       "  40,\n",
       "  8214,\n",
       "  44,\n",
       "  3606,\n",
       "  27,\n",
       "  367,\n",
       "  5,\n",
       "  129,\n",
       "  57,\n",
       "  20,\n",
       "  1,\n",
       "  26418,\n",
       "  12,\n",
       "  17,\n",
       "  48,\n",
       "  44,\n",
       "  24,\n",
       "  65,\n",
       "  330,\n",
       "  3,\n",
       "  2060,\n",
       "  5,\n",
       "  419,\n",
       "  20,\n",
       "  1,\n",
       "  1879,\n",
       "  15,\n",
       "  3,\n",
       "  3394,\n",
       "  199,\n",
       "  1,\n",
       "  32414,\n",
       "  24,\n",
       "  279,\n",
       "  66,\n",
       "  36,\n",
       "  3,\n",
       "  342,\n",
       "  1,\n",
       "  754,\n",
       "  697,\n",
       "  3,\n",
       "  3910,\n",
       "  1203,\n",
       "  20,\n",
       "  1,\n",
       "  1684,\n",
       "  3,\n",
       "  1206,\n",
       "  2,\n",
       "  277,\n",
       "  24,\n",
       "  279,\n",
       "  2577,\n",
       "  5,\n",
       "  67,\n",
       "  48,\n",
       "  29,\n",
       "  38,\n",
       "  5,\n",
       "  27,\n",
       "  3204,\n",
       "  11,\n",
       "  6,\n",
       "  24372,\n",
       "  15811,\n",
       "  32415,\n",
       "  12,\n",
       "  3698,\n",
       "  2802,\n",
       "  35,\n",
       "  4122,\n",
       "  35,\n",
       "  415,\n",
       "  14,\n",
       "  12712,\n",
       "  280,\n",
       "  3,\n",
       "  1002,\n",
       "  130,\n",
       "  35,\n",
       "  43,\n",
       "  277,\n",
       "  7,\n",
       "  1,\n",
       "  188,\n",
       "  344,\n",
       "  6872,\n",
       "  5,\n",
       "  92,\n",
       "  3,\n",
       "  2060,\n",
       "  16,\n",
       "  3,\n",
       "  5212,\n",
       "  2952,\n",
       "  15812,\n",
       "  21263,\n",
       "  5,\n",
       "  67,\n",
       "  44,\n",
       "  26,\n",
       "  68,\n",
       "  419,\n",
       "  7,\n",
       "  1,\n",
       "  1879,\n",
       "  15,\n",
       "  3699,\n",
       "  501,\n",
       "  199,\n",
       "  1,\n",
       "  32414,\n",
       "  44,\n",
       "  12712,\n",
       "  2803,\n",
       "  26,\n",
       "  68,\n",
       "  82,\n",
       "  48,\n",
       "  26,\n",
       "  467,\n",
       "  16,\n",
       "  3,\n",
       "  720,\n",
       "  1163,\n",
       "  4,\n",
       "  243,\n",
       "  50,\n",
       "  4393,\n",
       "  1,\n",
       "  9618,\n",
       "  20,\n",
       "  112,\n",
       "  12712,\n",
       "  6,\n",
       "  1295,\n",
       "  20,\n",
       "  1,\n",
       "  885,\n",
       "  16,\n",
       "  3,\n",
       "  20114,\n",
       "  20,\n",
       "  23,\n",
       "  4065,\n",
       "  5,\n",
       "  10545,\n",
       "  23,\n",
       "  167,\n",
       "  823,\n",
       "  112,\n",
       "  26,\n",
       "  172,\n",
       "  1554,\n",
       "  126,\n",
       "  1,\n",
       "  6480,\n",
       "  225,\n",
       "  330,\n",
       "  1,\n",
       "  13585,\n",
       "  44659,\n",
       "  32,\n",
       "  3,\n",
       "  37177,\n",
       "  100,\n",
       "  29,\n",
       "  422,\n",
       "  20,\n",
       "  23,\n",
       "  12323,\n",
       "  112,\n",
       "  12712,\n",
       "  874,\n",
       "  80,\n",
       "  101,\n",
       "  568,\n",
       "  3,\n",
       "  244,\n",
       "  32,\n",
       "  1,\n",
       "  400,\n",
       "  4,\n",
       "  4669,\n",
       "  20115,\n",
       "  2023,\n",
       "  3806,\n",
       "  33,\n",
       "  44660,\n",
       "  35,\n",
       "  182,\n",
       "  4327,\n",
       "  158,\n",
       "  2218,\n",
       "  39,\n",
       "  342,\n",
       "  2,\n",
       "  39,\n",
       "  8500,\n",
       "  7383,\n",
       "  2129,\n",
       "  4670,\n",
       "  2,\n",
       "  32416,\n",
       "  9399,\n",
       "  2592,\n",
       "  35,\n",
       "  22,\n",
       "  444,\n",
       "  327,\n",
       "  5,\n",
       "  1,\n",
       "  1879,\n",
       "  491,\n",
       "  4167,\n",
       "  12712,\n",
       "  203,\n",
       "  225,\n",
       "  21,\n",
       "  327,\n",
       "  5,\n",
       "  4432,\n",
       "  5908,\n",
       "  32417,\n",
       "  38,\n",
       "  26,\n",
       "  279,\n",
       "  116,\n",
       "  52,\n",
       "  106,\n",
       "  1002,\n",
       "  112,\n",
       "  29,\n",
       "  532,\n",
       "  40,\n",
       "  2817,\n",
       "  500,\n",
       "  40,\n",
       "  27,\n",
       "  14621,\n",
       "  12,\n",
       "  131,\n",
       "  1,\n",
       "  115,\n",
       "  1949,\n",
       "  191,\n",
       "  4669,\n",
       "  2,\n",
       "  12712,\n",
       "  267,\n",
       "  1664,\n",
       "  5,\n",
       "  113,\n",
       "  10,\n",
       "  248,\n",
       "  119,\n",
       "  4592,\n",
       "  5,\n",
       "  27,\n",
       "  28,\n",
       "  4,\n",
       "  3698,\n",
       "  2802,\n",
       "  17222,\n",
       "  95,\n",
       "  112,\n",
       "  2578,\n",
       "  5,\n",
       "  106,\n",
       "  3,\n",
       "  220,\n",
       "  8,\n",
       "  263,\n",
       "  3,\n",
       "  4206,\n",
       "  505,\n",
       "  1046,\n",
       "  5,\n",
       "  23,\n",
       "  2579,\n",
       "  161,\n",
       "  135,\n",
       "  14,\n",
       "  7945,\n",
       "  11978,\n",
       "  181,\n",
       "  5321,\n",
       "  40,\n",
       "  20116,\n",
       "  15,\n",
       "  1,\n",
       "  549,\n",
       "  5,\n",
       "  118,\n",
       "  48,\n",
       "  29,\n",
       "  38,\n",
       "  256,\n",
       "  139,\n",
       "  4394,\n",
       "  158,\n",
       "  2218,\n",
       "  8,\n",
       "  1,\n",
       "  367,\n",
       "  264,\n",
       "  40,\n",
       "  20,\n",
       "  1,\n",
       "  80,\n",
       "  544,\n",
       "  243,\n",
       "  3,\n",
       "  373,\n",
       "  2060,\n",
       "  38,\n",
       "  31,\n",
       "  1002,\n",
       "  81,\n",
       "  82,\n",
       "  52,\n",
       "  34,\n",
       "  89,\n",
       "  117,\n",
       "  48,\n",
       "  5,\n",
       "  82,\n",
       "  16,\n",
       "  64,\n",
       "  284,\n",
       "  270,\n",
       "  34,\n",
       "  138,\n",
       "  190,\n",
       "  8,\n",
       "  5,\n",
       "  1,\n",
       "  3204,\n",
       "  305,\n",
       "  4,\n",
       "  743,\n",
       "  8,\n",
       "  38,\n",
       "  18079,\n",
       "  7273,\n",
       "  12,\n",
       "  40,\n",
       "  270,\n",
       "  9,\n",
       "  19,\n",
       "  75,\n",
       "  5993,\n",
       "  24,\n",
       "  5,\n",
       "  325,\n",
       "  379]]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_train_reviews[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_test_reviews = combine_reviews(\"/Users/pugal/LeaseResearch/aclImdb/test/pos\")\n",
    "negative_test_reviews = combine_reviews(\"/Users/pugal/LeaseResearch/aclImdb/test/neg\")\n",
    "encoded_test_reviews = encode_reviews(positive_test_reviews, negative_test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[9,\n",
       "  390,\n",
       "  2,\n",
       "  200,\n",
       "  10,\n",
       "  17,\n",
       "  234,\n",
       "  320,\n",
       "  101,\n",
       "  107,\n",
       "  31818,\n",
       "  5,\n",
       "  32,\n",
       "  3,\n",
       "  166,\n",
       "  326,\n",
       "  4,\n",
       "  1670,\n",
       "  538,\n",
       "  960,\n",
       "  11,\n",
       "  9,\n",
       "  13,\n",
       "  5909,\n",
       "  5,\n",
       "  61,\n",
       "  8,\n",
       "  83,\n",
       "  35,\n",
       "  48,\n",
       "  9,\n",
       "  619,\n",
       "  4,\n",
       "  8354,\n",
       "  7792,\n",
       "  27,\n",
       "  13,\n",
       "  62,\n",
       "  445,\n",
       "  5,\n",
       "  79,\n",
       "  208,\n",
       "  9,\n",
       "  13,\n",
       "  359,\n",
       "  7792,\n",
       "  243,\n",
       "  1,\n",
       "  108,\n",
       "  4,\n",
       "  4082,\n",
       "  18588,\n",
       "  50,\n",
       "  73,\n",
       "  2,\n",
       "  1416,\n",
       "  6300,\n",
       "  243,\n",
       "  1564,\n",
       "  10517,\n",
       "  15,\n",
       "  136,\n",
       "  10801,\n",
       "  1,\n",
       "  2107,\n",
       "  4,\n",
       "  3,\n",
       "  49,\n",
       "  17,\n",
       "  6,\n",
       "  11,\n",
       "  8,\n",
       "  66,\n",
       "  3410,\n",
       "  15,\n",
       "  249,\n",
       "  1226,\n",
       "  10,\n",
       "  28,\n",
       "  113,\n",
       "  593,\n",
       "  11,\n",
       "  1,\n",
       "  427,\n",
       "  796,\n",
       "  64,\n",
       "  13,\n",
       "  3013,\n",
       "  43,\n",
       "  13,\n",
       "  3411,\n",
       "  32,\n",
       "  2253,\n",
       "  286,\n",
       "  1,\n",
       "  89,\n",
       "  331,\n",
       "  4,\n",
       "  1,\n",
       "  17,\n",
       "  2,\n",
       "  65,\n",
       "  1556,\n",
       "  5,\n",
       "  1731,\n",
       "  286,\n",
       "  1,\n",
       "  336,\n",
       "  331,\n",
       "  135,\n",
       "  11719,\n",
       "  1,\n",
       "  796,\n",
       "  9,\n",
       "  22,\n",
       "  62,\n",
       "  200,\n",
       "  103,\n",
       "  377,\n",
       "  7,\n",
       "  1731,\n",
       "  18,\n",
       "  103,\n",
       "  361,\n",
       "  2376,\n",
       "  332,\n",
       "  14,\n",
       "  73,\n",
       "  254,\n",
       "  2818,\n",
       "  22,\n",
       "  5,\n",
       "  367,\n",
       "  239,\n",
       "  61,\n",
       "  91,\n",
       "  2405,\n",
       "  10,\n",
       "  17,\n",
       "  13,\n",
       "  77,\n",
       "  2,\n",
       "  9,\n",
       "  1417,\n",
       "  11,\n",
       "  21,\n",
       "  140,\n",
       "  61,\n",
       "  8,\n",
       "  157,\n",
       "  21,\n",
       "  1501],\n",
       " [300,\n",
       "  644,\n",
       "  172,\n",
       "  987,\n",
       "  6972,\n",
       "  1040,\n",
       "  56,\n",
       "  24,\n",
       "  2355,\n",
       "  2099,\n",
       "  1,\n",
       "  43882,\n",
       "  17667,\n",
       "  15,\n",
       "  10,\n",
       "  248,\n",
       "  3475,\n",
       "  2444,\n",
       "  473,\n",
       "  41,\n",
       "  1,\n",
       "  14249,\n",
       "  168,\n",
       "  818,\n",
       "  114,\n",
       "  3,\n",
       "  192,\n",
       "  288,\n",
       "  14250,\n",
       "  6128,\n",
       "  35,\n",
       "  24,\n",
       "  5535,\n",
       "  925,\n",
       "  5,\n",
       "  277,\n",
       "  456,\n",
       "  24,\n",
       "  58885,\n",
       "  5849,\n",
       "  7,\n",
       "  48,\n",
       "  13,\n",
       "  2196,\n",
       "  14,\n",
       "  1,\n",
       "  755,\n",
       "  436,\n",
       "  119,\n",
       "  243,\n",
       "  147,\n",
       "  54,\n",
       "  311,\n",
       "  4,\n",
       "  3812,\n",
       "  2,\n",
       "  127,\n",
       "  22184,\n",
       "  5466,\n",
       "  2444,\n",
       "  1405,\n",
       "  23,\n",
       "  3,\n",
       "  12409,\n",
       "  3,\n",
       "  2762,\n",
       "  86,\n",
       "  1000,\n",
       "  221,\n",
       "  5,\n",
       "  1794,\n",
       "  905,\n",
       "  15,\n",
       "  5973,\n",
       "  2,\n",
       "  10802,\n",
       "  132,\n",
       "  18,\n",
       "  46,\n",
       "  84,\n",
       "  10,\n",
       "  19,\n",
       "  13,\n",
       "  10265,\n",
       "  31,\n",
       "  1,\n",
       "  10518,\n",
       "  12,\n",
       "  1,\n",
       "  19,\n",
       "  530,\n",
       "  15,\n",
       "  46,\n",
       "  1469,\n",
       "  580,\n",
       "  901,\n",
       "  781,\n",
       "  3,\n",
       "  43883,\n",
       "  322,\n",
       "  4,\n",
       "  1,\n",
       "  1438,\n",
       "  580,\n",
       "  901,\n",
       "  4,\n",
       "  13715,\n",
       "  36614,\n",
       "  2,\n",
       "  4836,\n",
       "  18,\n",
       "  23882,\n",
       "  315,\n",
       "  1305,\n",
       "  16,\n",
       "  29,\n",
       "  89,\n",
       "  14251,\n",
       "  565,\n",
       "  283,\n",
       "  1,\n",
       "  225,\n",
       "  1099,\n",
       "  5,\n",
       "  1,\n",
       "  168,\n",
       "  818,\n",
       "  175,\n",
       "  1127,\n",
       "  56,\n",
       "  50,\n",
       "  73,\n",
       "  6972,\n",
       "  122,\n",
       "  3,\n",
       "  330,\n",
       "  291,\n",
       "  2,\n",
       "  251,\n",
       "  3,\n",
       "  9789,\n",
       "  16,\n",
       "  1208,\n",
       "  3655,\n",
       "  15441,\n",
       "  9,\n",
       "  407,\n",
       "  1,\n",
       "  36615,\n",
       "  4111,\n",
       "  4,\n",
       "  1,\n",
       "  225,\n",
       "  20,\n",
       "  275,\n",
       "  109,\n",
       "  4,\n",
       "  1,\n",
       "  818,\n",
       "  11,\n",
       "  12410,\n",
       "  1,\n",
       "  115,\n",
       "  1080,\n",
       "  39,\n",
       "  680,\n",
       "  46,\n",
       "  2100,\n",
       "  1959,\n",
       "  1143,\n",
       "  5,\n",
       "  1,\n",
       "  4145,\n",
       "  213,\n",
       "  46,\n",
       "  1513,\n",
       "  108,\n",
       "  1011,\n",
       "  52,\n",
       "  1,\n",
       "  712,\n",
       "  1491,\n",
       "  11720,\n",
       "  6,\n",
       "  2668,\n",
       "  32,\n",
       "  1023,\n",
       "  4,\n",
       "  1,\n",
       "  19601,\n",
       "  7,\n",
       "  307,\n",
       "  2909,\n",
       "  2,\n",
       "  391,\n",
       "  5011,\n",
       "  36,\n",
       "  2343,\n",
       "  24,\n",
       "  248,\n",
       "  10266,\n",
       "  14,\n",
       "  3,\n",
       "  602,\n",
       "  5,\n",
       "  92,\n",
       "  99,\n",
       "  16,\n",
       "  3,\n",
       "  3812,\n",
       "  263,\n",
       "  27,\n",
       "  80,\n",
       "  122,\n",
       "  3,\n",
       "  49,\n",
       "  291,\n",
       "  4,\n",
       "  2254,\n",
       "  4396,\n",
       "  48,\n",
       "  266,\n",
       "  20,\n",
       "  7,\n",
       "  1,\n",
       "  1821,\n",
       "  1725,\n",
       "  483,\n",
       "  4712,\n",
       "  3812,\n",
       "  3,\n",
       "  2503,\n",
       "  340,\n",
       "  3944,\n",
       "  6,\n",
       "  786,\n",
       "  8819,\n",
       "  1049,\n",
       "  137,\n",
       "  1061,\n",
       "  139,\n",
       "  80,\n",
       "  26,\n",
       "  349,\n",
       "  1,\n",
       "  289,\n",
       "  10803,\n",
       "  2,\n",
       "  2532,\n",
       "  2261,\n",
       "  16,\n",
       "  1775,\n",
       "  33,\n",
       "  1891,\n",
       "  43884,\n",
       "  862,\n",
       "  4,\n",
       "  1351,\n",
       "  2,\n",
       "  6649,\n",
       "  30,\n",
       "  1,\n",
       "  439,\n",
       "  4,\n",
       "  1,\n",
       "  9371,\n",
       "  15442,\n",
       "  12,\n",
       "  21,\n",
       "  111,\n",
       "  84,\n",
       "  10,\n",
       "  6,\n",
       "  160,\n",
       "  5,\n",
       "  129,\n",
       "  22,\n",
       "  62,\n",
       "  83,\n",
       "  29,\n",
       "  443,\n",
       "  20,\n",
       "  3,\n",
       "  292,\n",
       "  68,\n",
       "  18,\n",
       "  80,\n",
       "  83,\n",
       "  94,\n",
       "  7,\n",
       "  10,\n",
       "  566,\n",
       "  900,\n",
       "  1,\n",
       "  161,\n",
       "  13716,\n",
       "  131,\n",
       "  2,\n",
       "  131,\n",
       "  18,\n",
       "  6972,\n",
       "  1260,\n",
       "  20,\n",
       "  3,\n",
       "  123,\n",
       "  70,\n",
       "  859,\n",
       "  117,\n",
       "  2,\n",
       "  386,\n",
       "  9166,\n",
       "  51,\n",
       "  605,\n",
       "  525,\n",
       "  1,\n",
       "  371,\n",
       "  70,\n",
       "  27,\n",
       "  119,\n",
       "  67,\n",
       "  7,\n",
       "  929,\n",
       "  4,\n",
       "  8,\n",
       "  461,\n",
       "  1,\n",
       "  4713,\n",
       "  968,\n",
       "  10,\n",
       "  6,\n",
       "  3,\n",
       "  330,\n",
       "  2,\n",
       "  759,\n",
       "  19,\n",
       "  5,\n",
       "  3747,\n",
       "  16,\n",
       "  11,\n",
       "  983,\n",
       "  5,\n",
       "  156,\n",
       "  33,\n",
       "  302]]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_test_reviews[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive testing reviews:  12500\n",
      "Number of negative testing reviews:  12500\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of positive testing reviews: \",len(positive_test_reviews))\n",
    "print(\"Number of negative testing reviews: \", len(negative_test_reviews))\n",
    "#25000 reviews, but website says 25001\n",
    "print(\"Total number of testing reviews: \", positive_test_reviews + negative_test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "count    25000.000000\n",
       "mean       232.841800\n",
       "std        173.067376\n",
       "min         10.000000\n",
       "25%        126.000000\n",
       "50%        174.000000\n",
       "75%        283.000000\n",
       "max       2469.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#making histogram of number of words in each training review\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "encoded_train_reviews_len = [len(x) for x in encoded_train_reviews]\n",
    "pd.Series(encoded_train_reviews_len).hist()\n",
    "plt.show()\n",
    "pd.Series(encoded_train_reviews_len).describe()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[138, 428, 147]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_train_reviews_len[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min isn't 0 so don't need to remove short reviews\n"
     ]
    }
   ],
   "source": [
    "print(\"min isn't 0 so don't need to remove short reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 9a: filter reviews\n",
    "def filter_reviews(reviews_int, small_cutoff, large_cutoff, reviews_len):\n",
    "    return [reviews_int[i] for i, l in enumerate(reviews_len) if l< large_cutoff and l > small_cutoff ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 9b: filter labels\n",
    "def filter_labels(labels, small_cutoff, large_cutoff, reviews_len):\n",
    "    return [labels[i] for i, l in enumerate(reviews_len) if l< large_cutoff and l > small_cutoff ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1]\n",
      "22990\n"
     ]
    }
   ],
   "source": [
    "#only have labels for reviews between 0 and 500 words\n",
    "filtered_train_labels = filter_labels(train_labels, 0, 500, encoded_train_reviews_len)\n",
    "print(filtered_train_labels[0:3])\n",
    "print(len(filtered_train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "count    22990.000000\n",
       "mean       193.002262\n",
       "std        101.725504\n",
       "min         10.000000\n",
       "25%        124.000000\n",
       "50%        164.000000\n",
       "75%        245.000000\n",
       "max        499.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#only have reviews between 0 and 500 words\n",
    "filtered_train_reviews = filter_reviews(encoded_train_reviews, 0, 500, encoded_train_reviews_len)\n",
    "%matplotlib inline\n",
    "filtered_train_reviews_len = [len(x) for x in filtered_train_reviews]\n",
    "pd.Series(filtered_train_reviews_len).hist()\n",
    "plt.show()\n",
    "pd.Series(filtered_train_reviews_len).describe()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 10 Padding/Truncating the remaining data\n",
    "def padding_truncating(reviews_int, seq_length):\n",
    "    ''' Return features of review_ints, where each review is padded with 0's or truncated to the input seq_length.\n",
    "    '''\n",
    "    import numpy as np\n",
    "    features = np.zeros((len(reviews_int), seq_length), dtype = int)\n",
    "    \n",
    "    for i, review in enumerate(reviews_int):\n",
    "        review_len = len(review)\n",
    "        \n",
    "        if review_len <= seq_length:\n",
    "            zeroes = list(np.zeros(seq_length-review_len))\n",
    "            new = zeroes+review\n",
    "        elif review_len > seq_length:\n",
    "            new = review[0:seq_length]\n",
    "        \n",
    "        features[i,:] = np.array(new)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0, 24371,\n",
       "          320,     6,     3,  1079,   220,     8,  2072,    30,     1,\n",
       "          166,    62,    14,    46,    80,  5722,    42,   399,   119,\n",
       "          135,    14,  4852,    55,  4947,   147,     7,     1,  4908,\n",
       "         5992,   474,    69,     5,   253,    11, 24371, 17221,  1963,\n",
       "            6,    72,  2356,     5,   632,    70,     6,  4852,     1,\n",
       "        26417,     5,  2021, 11069,     1,  5846,  1422,    35,    68,\n",
       "           67,   206,   140,    64,  1228,  4852, 21261,     1, 44658,\n",
       "            4,     1,   214,   901,    31,  2898,    69,     4,     1,\n",
       "         4668,    10,   661,     2,    64,  1422,    52,    10,   207,\n",
       "            1,   382,     7,    59,     3,  1470,  3605,   767,     5,\n",
       "         3555,   187,     1,   399,    10,  1192, 15181,    30,   320,\n",
       "            3,   353,   361,  2982,   144,   133,     5,  9212,    28,\n",
       "            4,   122,  4852,  1470,  2500,     5, 24371,   320,    10,\n",
       "          510,    11,   104,  1476,     4,    55,   582,   102,    11,\n",
       "        24371,   320,     6,   231,  8838,    48,     3,  2305,    11,\n",
       "            8,   203],\n",
       "       [26421,    40, 59564,    14,   723, 20113,  3360,    43,    73,\n",
       "           33,  1845,    15,   147,    17,   109,     3,  1284,     5,\n",
       "          325,   142,    20,     1,   885,    11,    65,   279,  1138,\n",
       "          398,    35,   116,   277,    36,   165,     5,   399,   161,\n",
       "           40,  2279,    15,     1,   549,    86,    81,   102,     4,\n",
       "            1,  3204,    14,    41,     3,   410,  1071,   131,  8214,\n",
       "           42,   178,   135,    14,  3095,     1,   328,    20,  4909,\n",
       "        29053,   326,     5,  3220,  2101,     1, 22694, 21262,    40,\n",
       "         8214,    44,  3606,    27,   367,     5,   129,    57,    20,\n",
       "            1, 26418,    12,    17,    48,    44,    24,    65,   330,\n",
       "            3,  2060,     5,   419,    20,     1,  1879,    15,     3,\n",
       "         3394,   199,     1, 32414,    24,   279,    66,    36,     3,\n",
       "          342,     1,   754,   697,     3,  3910,  1203,    20,     1,\n",
       "         1684,     3,  1206,     2,   277,    24,   279,  2577,     5,\n",
       "           67,    48,    29,    38,     5,    27,  3204,    11,     6,\n",
       "        24372, 15811, 32415,    12,  3698,  2802,    35,  4122,    35,\n",
       "          415,    14, 12712,   280,     3,  1002,   130,    35,    43,\n",
       "          277,     7,     1,   188,   344,  6872,     5,    92,     3,\n",
       "         2060,    16,     3,  5212,  2952, 15812, 21263,     5,    67,\n",
       "           44,    26,    68,   419,     7,     1,  1879,    15,  3699,\n",
       "          501,   199,     1, 32414,    44, 12712,  2803,    26,    68,\n",
       "           82,    48,    26,   467,    16,     3,   720,  1163,     4,\n",
       "          243,    50],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,   524,\n",
       "         3807,    32, 20115,  2023,  3806,   114,   902, 26419,   737,\n",
       "           10,    25,   121,   108,     2,   115,   136,     7,  1647,\n",
       "         8090,    22,   324,     5,   584,     1,  7079,    20,   403,\n",
       "            6,     3,   353,    14,    49,    14,   228,     7,  7945,\n",
       "        11978,     1,   186,    20,  8839,     6,    78,   904,   100,\n",
       "          106,  3510,     4,   106,     3, 32418,  3844,    45,    23,\n",
       "         1345,     2,   106, 13586,     1,  2337,     4, 44659, 12712,\n",
       "        26420, 37178,   144,     3,  2337,    26,   533,   291,   162,\n",
       "          699,  4090, 21263,     3,   496,    36,     1,   299,  2758,\n",
       "         7173,   118,     6,   770,   133,    99,    14,     3,  1158,\n",
       "         5723,    35,   467,     5,  4284,     1,  6674,    23,   107,\n",
       "            6,    50, 10806,    70,   611,     1,  1513,   132,     2,\n",
       "            1,   132,   112,     1,  3204, 18080,     3, 14622,  2119,\n",
       "           22,  3808,  2180,   164,    15,     1,  2996,   132,     2,\n",
       "            1,   105,   194, 17223,   984,    28, 29054,     9,    18,\n",
       "          200,   125,   245,    62,    10,    67,     8,    59,     6,\n",
       "          174,   388]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_encoded_training_data = padding_truncating(filtered_train_reviews,200)\n",
    "final_encoded_training_data[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "count    25000.000000\n",
       "mean       227.609240\n",
       "std        168.185625\n",
       "min          4.000000\n",
       "25%        125.000000\n",
       "50%        171.000000\n",
       "75%        276.000000\n",
       "max       2272.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's visualise the encoded test reviews\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "encoded_test_reviews_len = [len(x) for x in encoded_test_reviews]\n",
    "pd.Series(encoded_test_reviews_len).hist()\n",
    "plt.show()\n",
    "pd.Series(encoded_test_reviews_len).describe()\n",
    "#use similar cutoffs for encoded test reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "count    23120.000000\n",
       "mean       190.499048\n",
       "std        100.640815\n",
       "min          4.000000\n",
       "25%        123.000000\n",
       "50%        163.000000\n",
       "75%        240.000000\n",
       "max        499.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#only have test reviews between 0 and 500 words\n",
    "filtered_test_reviews = filter_reviews(encoded_test_reviews, 0, 500, encoded_test_reviews_len)\n",
    "%matplotlib inline\n",
    "filtered_test_reviews_len = [len(x) for x in filtered_test_reviews]\n",
    "pd.Series(filtered_test_reviews_len).hist()\n",
    "plt.show()\n",
    "pd.Series(filtered_test_reviews_len).describe()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     9,   390,     2,   200,    10,    17,\n",
       "          234,   320,   101,   107, 31818,     5,    32,     3,   166,\n",
       "          326,     4,  1670,   538,   960,    11,     9,    13,  5909,\n",
       "            5,    61,     8,    83,    35,    48,     9,   619,     4,\n",
       "         8354,  7792,    27,    13,    62,   445,     5,    79,   208,\n",
       "            9,    13,   359,  7792,   243,     1,   108,     4,  4082,\n",
       "        18588,    50,    73,     2,  1416,  6300,   243,  1564, 10517,\n",
       "           15,   136, 10801,     1,  2107,     4,     3,    49,    17,\n",
       "            6,    11,     8,    66,  3410,    15,   249,  1226,    10,\n",
       "           28,   113,   593,    11,     1,   427,   796,    64,    13,\n",
       "         3013,    43,    13,  3411,    32,  2253,   286,     1,    89,\n",
       "          331,     4,     1,    17,     2,    65,  1556,     5,  1731,\n",
       "          286,     1,   336,   331,   135, 11719,     1,   796,     9,\n",
       "           22,    62,   200,   103,   377,     7,  1731,    18,   103,\n",
       "          361,  2376,   332,    14,    73,   254,  2818,    22,     5,\n",
       "          367,   239,    61,    91,  2405,    10,    17,    13,    77,\n",
       "            2,     9,  1417,    11,    21,   140,    61,     8,   157,\n",
       "           21,  1501],\n",
       "       [  300,   644,   172,   987,  6972,  1040,    56,    24,  2355,\n",
       "         2099,     1, 43882, 17667,    15,    10,   248,  3475,  2444,\n",
       "          473,    41,     1, 14249,   168,   818,   114,     3,   192,\n",
       "          288, 14250,  6128,    35,    24,  5535,   925,     5,   277,\n",
       "          456,    24, 58885,  5849,     7,    48,    13,  2196,    14,\n",
       "            1,   755,   436,   119,   243,   147,    54,   311,     4,\n",
       "         3812,     2,   127, 22184,  5466,  2444,  1405,    23,     3,\n",
       "        12409,     3,  2762,    86,  1000,   221,     5,  1794,   905,\n",
       "           15,  5973,     2, 10802,   132,    18,    46,    84,    10,\n",
       "           19,    13, 10265,    31,     1, 10518,    12,     1,    19,\n",
       "          530,    15,    46,  1469,   580,   901,   781,     3, 43883,\n",
       "          322,     4,     1,  1438,   580,   901,     4, 13715, 36614,\n",
       "            2,  4836,    18, 23882,   315,  1305,    16,    29,    89,\n",
       "        14251,   565,   283,     1,   225,  1099,     5,     1,   168,\n",
       "          818,   175,  1127,    56,    50,    73,  6972,   122,     3,\n",
       "          330,   291,     2,   251,     3,  9789,    16,  1208,  3655,\n",
       "        15441,     9,   407,     1, 36615,  4111,     4,     1,   225,\n",
       "           20,   275,   109,     4,     1,   818,    11, 12410,     1,\n",
       "          115,  1080,    39,   680,    46,  2100,  1959,  1143,     5,\n",
       "            1,  4145,   213,    46,  1513,   108,  1011,    52,     1,\n",
       "          712,  1491, 11720,     6,  2668,    32,  1023,     4,     1,\n",
       "        19601,     7,   307,  2909,     2,   391,  5011,    36,  2343,\n",
       "           24,   248],\n",
       "       [   14,     3, 25976, 23883,    15,    46,  1750,     4,     1,\n",
       "         2444,   497,     9,    13,  3877,    15,  6559,  7403,     5,\n",
       "            1,  1157,     4,   823,     7,  3812,     7,     1,   405,\n",
       "         9371,  1095,     1,    17,  2268,    73,     1,  1959,  3719,\n",
       "           11,  1491, 11720,  4083,   745,   314,    35,    24,  1716,\n",
       "         6861,     4,   107, 31819,     5,    24,   196,  5147,     5,\n",
       "         1075,    11,  2878,  6973,    11,  9790,    88,    35,   107,\n",
       "         2616,    14,    33,  3152,     7,   598,  3812,   908,  4917,\n",
       "            1,   192, 14252,   266,   141,    24,   196,   823,  2930,\n",
       "          107,     3,  2819, 43885,     7,     1,   516,     4,     1,\n",
       "         3977, 16092,  1527,    36, 23884,    30,    24,   972,     5,\n",
       "         2197,   722,    24,  2170,    12,    12,    48,     9,   407,\n",
       "          116,   186,     6,    84,    10,   770,     4,   823,     6,\n",
       "        20817,     7,     1,    97,     4, 23885,   709,    24,   389,\n",
       "            6,     3, 14836, 16831,    36,  1045,     1,  1096,     4,\n",
       "          262,   165,    18,     6, 25977,    32,     1,  3977,   823,\n",
       "           24,   403,   186, 12053,    45,  2820,   605,     2,  2181,\n",
       "            2,  8820,    88,     5,  6650,    24,  1081,     4,  7672,\n",
       "          456,   138,    36,   102,    27,     6, 58887,    12,   414,\n",
       "            1,  3812,   130,    23,    73,  3656,   264,     1,   263,\n",
       "          310,     7,     1,    17,    13,    22,     1,   816,  2035,\n",
       "            4,     1,  1221, 12411,     1,   118,  6651,   558,    32,\n",
       "         1212,    79]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_encoded_testing_data = padding_truncating(filtered_test_reviews,200)\n",
    "final_encoded_testing_data[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1]\n",
      "23120\n"
     ]
    }
   ],
   "source": [
    "#only have labels for reviews between 0 and 500 words\n",
    "test_labels = encode_labels(12500, 12500)\n",
    "filtered_test_labels = filter_labels(test_labels, 0, 500, encoded_test_reviews_len)\n",
    "print(filtered_test_labels[0:3])\n",
    "print(len(filtered_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting aside 20% of the training reviews to make a seperate list of 'valid' reviews\n",
    "split_frac = 0.8\n",
    "len_feat = len(final_encoded_training_data)\n",
    "train_x = final_encoded_training_data[0:int(split_frac*len_feat)]\n",
    "train_y = filtered_train_labels[0:int(split_frac*len_feat)]\n",
    "valid_x = final_encoded_training_data[int(split_frac*len_feat):]\n",
    "valid_y = filtered_train_labels[int(split_frac*len_feat):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 12 Dataloaders and Batching, adapted from article\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# create Tensor datasets\n",
    "#torch.from_numpy requires an array, so np.array used to transform list of labels into an array of labels\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(np.array(train_y)))\n",
    "test_data = TensorDataset(torch.from_numpy(final_encoded_testing_data), torch.from_numpy(np.array(filtered_test_labels)))\n",
    "valid_data = TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(np.array(valid_y)))\n",
    "# dataloaders\n",
    "batch_size = 50\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 14: Define LSTM class\n",
    "import torch.nn as nn\n",
    "\n",
    "class SentimentLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "    \n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        #if (train_on_gpu):\n",
    "         #   hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "          #        weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        #else:\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                    weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vocab(an_array):\n",
    "    return sum(len(row) for row in an_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentLSTM(\n",
      "  (embedding): Embedding(3678401, 400)\n",
      "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the network\n",
    "# Instantiate the model w/ hyperparams\n",
    "vocab_size = count_vocab(train_x)+1 # +1 for the 0 padding\n",
    "output_size = 1\n",
    "embedding_dim = 400\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "net = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-9951816ab692>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;31m# calculate the loss and perform backprop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;31m# `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\IntroCL\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \"\"\"\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\IntroCL\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "# training params\n",
    "\n",
    "epochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "# move model to GPU, if available\n",
    "#if(train_on_gpu):\n",
    "    #net.cuda()\n",
    "\n",
    "    \n",
    "net.train()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    #how many times has the graph been backtraced?\n",
    "    #temp = 0    \n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "\n",
    "        #if(train_on_gpu):\n",
    "         #   inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        #h = tuple([each.data for each in h])\n",
    "\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        inputs = inputs.type(torch.LongTensor)\n",
    "        output, h = net(inputs, h)\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            # Get validation loss\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                #if(train_on_gpu):\n",
    "                    #inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                inputs = inputs.type(torch.LongTensor)\n",
    "                output, val_h = net(inputs, val_h)\n",
    "                val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            net.train()\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-cd2f40bb2000>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m             \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;31m# `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\IntroCL\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \"\"\"\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\IntroCL\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "# training params\n",
    "\n",
    "epochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "# move model to GPU, if available\n",
    "#if(train_on_gpu):\n",
    "    #net.cuda()\n",
    "\n",
    "    \n",
    "net.train()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    #how many times has the graph been backtraced?\n",
    "    temp = 0    \n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "\n",
    "        #if(train_on_gpu):\n",
    "         #   inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        #h = tuple([each.data for each in h])\n",
    "\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        inputs = inputs.type(torch.LongTensor)\n",
    "        output, h = net(inputs, h)\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        if temp == 1:\n",
    "            loss.backward()\n",
    "        else:\n",
    "            loss.backward(retain_graph=True)\n",
    "            temp = 1\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            # Get validation loss\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                #if(train_on_gpu):\n",
    "                    #inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                inputs = inputs.type(torch.LongTensor)\n",
    "                output, val_h = net(inputs, val_h)\n",
    "                val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            net.train()\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
